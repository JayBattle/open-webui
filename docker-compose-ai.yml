version: '3.9'

# 1. Define the AI Model as a top-level element
models:
  # The name you'll use to reference the model in your services
  smollm_model:
    # The official Docker Hub image identifier for SmolLM2
    model: ai/smollm2:360M-Q4_K_M # Using a specific variant for stability
    # Optional: Customize the model's context size if needed
    # context_size: 4096 
    # Optional: Pass raw flags to the underlying inference engine (e.g., llama.cpp)
    # runtime_flags:
    #   - "--temp=0.7"
    #   - "--top-k=40"

# 2. Define your application services
services:
  chat-app:
    # Replace with your actual application image
    image: my-chat-application:latest 
    ports:
      - "8080:8080" # Example port for your app

    # Bind the defined model to this service
    models:
      - smollm_model

    # The Docker Model Runner automatically injects environment variables
    # to connect your app to the model's OpenAI-compatible endpoint.
    environment:
      # These variables are automatically generated based on the model name:
      # <MODEL_NAME_UPPERCASE>_URL and <MODEL_NAME_UPPERCASE>_MODEL
      LLM_ENDPOINT_URL: ${SMOLLM_MODEL_URL}
      LLM_MODEL_NAME: ${SMOLLM_MODEL_MODEL}